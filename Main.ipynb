{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ayami-n/Flax_text_prediction/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2DeZbEYMnwWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658471515112,"user_tz":-600,"elapsed":49284,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"2e15043d-788a-4598-ba67-b9bf43e370a2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Flax_text_prediction"],"metadata":{"id":"qhezgqRvnzTS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658471532522,"user_tz":-600,"elapsed":1082,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"f69a3bc7-6d2a-49f3-c412-31e45ebb419d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Flax_text_prediction\n"]}]},{"cell_type":"markdown","source":["# Import libs"],"metadata":{"id":"NRKyEoECnnSd"}},{"cell_type":"code","source":["%%capture\n","!pip install datasets\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install flax\n","!pip install git+https://github.com/deepmind/optax.git"],"metadata":{"id":"Iv2TRPsJpyAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import jax\n","from jax import random  # to create random values for initalizing a model (Flax requires)\n","import jax.numpy as jnp\n","\n","# Flax for building model\n","try:\n","    import flax\n","except ModuleNotFoundError: # Install flax if missing\n","    !pip install --quiet flax\n","    import flax\n","\n","from flax import linen as nn\n","from flax.training import train_state, checkpoints\n","from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n","\n","# Optax for optimizor \n","import optax\n","\n","# Transformers\n","!pip install transformers\n","from transformers import FlaxAutoModelForSequenceClassification, RobertaTokenizer, RobertaConfig # as we use Roberta model\n","from transformers.modeling_flax_utils import FlaxPreTrainedModel  # FlaxMLPModule is still stateless\n","\n","# others\n","import pandas as pd\n","from tqdm import tqdm\n","from typing import Callable\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"epCnK0nRnmhW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658471557901,"user_tz":-600,"elapsed":21789,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"64006925-5b3e-4cb6-d5c1-2d1cd602d9f5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 197 kB 32.8 MB/s \n","\u001b[K     |████████████████████████████████| 217 kB 70.9 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 70.0 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 65.4 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 7.0 MB/s \n","\u001b[K     |████████████████████████████████| 72 kB 589 kB/s \n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 36.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"IVImuz8mpJXl"}},{"cell_type":"code","source":["model_checkpoint = 'siebert/sentiment-roberta-large-english' # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n","num_labels = 3 # our targets\n","seed = 0  # for building our model\n","\n","myConfig = RobertaConfig(num_labels=num_labels)  # create a config for our model\n","tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)  # this tokenizer converts numeric from string: the values are different if you select different model_checkpoint"],"metadata":{"id":"roAJE2JZpLjv","executionInfo":{"status":"ok","timestamp":1658471937602,"user_tz":-600,"elapsed":5737,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Tokenaization"],"metadata":{"id":"zduhy-t_m7q1"}},{"cell_type":"code","source":["'''\n","https://stackoverflow.com/questions/65246703/how-does-max-length-padding-and-truncation-arguments-work-in-huggingface-bertt\n","adding [CLS] token at the beginning of the sentence, and [SEP] token at the end of sentence.\n","[CLS] I love you [SEP] is expected by BERT. \n","tokenizer gives [CLS] and [SEP] usually\n","'''"],"metadata":{"id":"YDDiF-Nkv-qH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"./kaggle/train.csv\")  # import train datasets\n","new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}  # to replace \n","df = df.replace(new_label) "],"metadata":{"id":"MBotyWi7nFlv","executionInfo":{"status":"ok","timestamp":1658471624783,"user_tz":-600,"elapsed":1323,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["max = 0\n","max_str = \"Who is Max?\"\n","words_list = []\n","for itr, val in enumerate(df['discourse_text'].to_numpy()):\n","  words = val.split()\n","  words_list.append(len(words))\n","\n","  if len(words) > max:\n","    max = len(words)\n","    max_str = val\n","\n","print(pd.Series(list(filter(lambda x: (x <= 256), words_list))).count(), len(words_list))    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1Xx753jpyup","executionInfo":{"status":"ok","timestamp":1658211808897,"user_tz":-600,"elapsed":519,"user":{"displayName":"Ayami Nonaka","userId":"17110434711451596966"}},"outputId":"958f0c70-499a-4e57-b0d8-74d06a551ded"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["36566 36765\n"]}]},{"cell_type":"code","source":["'''\n","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.kde.html\n","'''\n","pd.Series(words_list).plot.kde()  # most words are less than 250"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"qYQS1fZEI0Ip","executionInfo":{"status":"ok","timestamp":1658212132122,"user_tz":-600,"elapsed":724,"user":{"displayName":"Ayami Nonaka","userId":"17110434711451596966"}},"outputId":"9a8ac7a9-aebd-4d25-8ed6-e4a08ceb4c5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7ff1f7bf1410>"]},"metadata":{},"execution_count":36},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["max_len = 256  # input text should be the same length (most words <= 256)\n","\n","def tokenaize(texts, tokenizer, max_len):  # df text comes and return as numerical \n","    input_ids = []\n","    attention_mask = []\n","    \n","    for text in tqdm(texts):  # it takes over 1 min (numpy and list)\n","        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length', return_tensors='jax') # add_special_tokens=True is default: truncation=True is cutting off longer sentences (longer than max_length)\n","        input_ids.append(token['input_ids']) # separate: the values (text) are converted by the tokenizer\n","        attention_mask.append(token['attention_mask']) # separate: the values (attention-mask) are converted by the tokenizer\n","        \n","    return jnp.array(input_ids), jnp.array(attention_mask)"],"metadata":{"id":"DobpcytJn15n","executionInfo":{"status":"ok","timestamp":1658471631479,"user_tz":-600,"elapsed":486,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_df, val_df = train_test_split(df, test_size=0.05, random_state=seed)  # spliting datasets 95% train and 5% val\n","\n","train_input_ids, train_attention_mask = tokenaize(train_df['discourse_text'].to_numpy()[:2000], tokenizer, max_len)  # pandas is slow -> numpy is the fastest (list is considerable)\n","val_input_ids, val_attention_mask = tokenaize(val_df['discourse_text'].to_numpy()[:50], tokenizer, max_len)"],"metadata":{"id":"FqZ_TapDqXwR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658471708923,"user_tz":-600,"elapsed":6317,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"c4ba34b7-477d-4474-f3b1-280ed3431da2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:04<00:00, 439.61it/s]\n","100%|██████████| 50/50 [00:00<00:00, 417.85it/s]\n"]}]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens(train_input_ids[0].squeeze())[:30]  # checking the converted id's"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWpACWVPZt28","executionInfo":{"status":"ok","timestamp":1658471717567,"user_tz":-600,"elapsed":1435,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"e2d21dbe-c277-4b36-e526-98db6941c9be"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>',\n"," 'In',\n"," 'Ġconclusion',\n"," ',',\n"," 'Ġthe',\n"," 'ĠElectoral',\n"," 'ĠCollege',\n"," 'Ġshould',\n"," 'Ġbe',\n"," 'Ġkept',\n"," '.',\n"," 'ĠIt',\n"," 'Ġinduces',\n"," 'Ġthe',\n"," 'Ġcandidates',\n"," '.',\n"," 'ĠIt',\n"," 'Ġrestores',\n"," 'Ġsome',\n"," 'Ġof',\n"," 'Ġthe',\n"," 'Ġweight',\n"," 'Ġthat',\n"," 'Ġthe',\n"," 'Ġlarge',\n"," 'Ġstates',\n"," 'Ġloses',\n"," '.',\n"," 'ĠIt',\n"," 'Ġavoids']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Create a model"],"metadata":{"id":"2FC74pNhnDIW"}},{"cell_type":"code","source":["train_label = jnp.array(train_df['discourse_effectiveness'].to_numpy())  # jnp (DeviceArray) as model requires\n","val_label = jnp.array(val_df['discourse_effectiveness'].to_numpy())"],"metadata":{"id":"fPeIbMvyKka1","executionInfo":{"status":"ok","timestamp":1658474078671,"user_tz":-600,"elapsed":2,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# the model size is 1.32G\n","model = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=myConfig, seed=seed, ignore_mismatched_sizes=True)  "],"metadata":{"id":"Z0V58UuSs4EW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658473794977,"user_tz":-600,"elapsed":5673,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}},"outputId":"b9a91149-61c0-4a24-ff62-ffa6eae8e9eb"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at siebert/sentiment-roberta-large-english were not used when initializing FlaxRobertaForSequenceClassification: {('roberta', 'encoder', 'layer', '17', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '15', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '13', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '16', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '18', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '22', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '15', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '21', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '16', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '17', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '16', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '17', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '15', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '21', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '12', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '13', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '20', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '21', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '15', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '13', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '13', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '23', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '21', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '21', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '18', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '17', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '18', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '21', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '19', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '19', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '20', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '12', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '20', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '12', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '13', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '22', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '17', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '20', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '15', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '18', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '12', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '18', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '12', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '21', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '12', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '23', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '17', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '18', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '19', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '20', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '19', 'attention', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '22', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '14', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '21', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '16', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '21', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '15', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'value', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '15', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '19', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '20', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '19', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '20', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '12', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '17', 'attention', 'self', 'query', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '21', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '23', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '16', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '13', 'attention', 'self', 'key', 'bias'), ('roberta', 'encoder', 'layer', '13', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '17', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'attention', 'output', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '19', 'attention', 'output', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '12', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'query', 'bias'), ('roberta', 'encoder', 'layer', '20', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '21', 'attention', 'self', 'key', 'kernel'), ('roberta', 'encoder', 'layer', '20', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '16', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '17', 'intermediate', 'dense', 'kernel'), ('roberta', 'encoder', 'layer', '22', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '23', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '22', 'output', 'LayerNorm', 'scale'), ('roberta', 'encoder', 'layer', '15', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '23', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '13', 'intermediate', 'dense', 'bias'), ('roberta', 'encoder', 'layer', '14', 'attention', 'self', 'value', 'bias'), ('roberta', 'encoder', 'layer', '20', 'attention', 'output', 'LayerNorm', 'bias'), ('roberta', 'encoder', 'layer', '18', 'attention', 'self', 'value', 'kernel')}\n","- This IS expected if you are initializing FlaxRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing FlaxRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of FlaxRobertaForSequenceClassification were not initialized from the model checkpoint at siebert/sentiment-roberta-large-english and are newly initialized because the shapes did not match:\n","- ('classifier', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('classifier', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('classifier', 'out_proj', 'bias'): found shape (2,) in the checkpoint and (3,) in the model instantiated\n","- ('classifier', 'out_proj', 'kernel'): found shape (1024, 2) in the checkpoint and (768, 3) in the model instantiated\n","- ('roberta', 'embeddings', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'embeddings', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'embeddings', 'position_embeddings', 'embedding'): found shape (514, 1024) in the checkpoint and (512, 768) in the model instantiated\n","- ('roberta', 'embeddings', 'token_type_embeddings', 'embedding'): found shape (1, 1024) in the checkpoint and (2, 768) in the model instantiated\n","- ('roberta', 'embeddings', 'word_embeddings', 'embedding'): found shape (50265, 1024) in the checkpoint and (30522, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '0', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '1', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '10', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '11', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '2', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '3', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '4', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '5', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '6', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '7', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '8', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'kernel'): found shape (1024, 1024) in the checkpoint and (768, 768) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'intermediate', 'dense', 'bias'): found shape (4096,) in the checkpoint and (3072,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'intermediate', 'dense', 'kernel'): found shape (1024, 4096) in the checkpoint and (768, 3072) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'scale'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'output', 'dense', 'bias'): found shape (1024,) in the checkpoint and (768,) in the model instantiated\n","- ('roberta', 'encoder', 'layer', '9', 'output', 'dense', 'kernel'): found shape (4096, 1024) in the checkpoint and (3072, 768) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["## Define the training state"],"metadata":{"id":"nSXdLv2_Lsec"}},{"cell_type":"code","source":["params = model.params\n","lr = 2e-5  # 0.00002\n","opt = optax.adamw(learning_rate=lr, b1=0.9, b2=0.999, eps=1e-6)\n","opt_state = opt.init(params)"],"metadata":{"id":"jwh_crZ-LULn","executionInfo":{"status":"ok","timestamp":1658473678698,"user_tz":-600,"elapsed":3,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def loss_function(logits, labels):  \n","  xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n","  return jnp.mean(xentropy)\n","\n","def compute_loss(params, input_ids, labels):\n","   logits = model(input_ids, params=params, train=True)\n","   num_classes = logits.shape[-1]\n","   loss = loss_function(logits, labels).mean()\n","   return loss  "],"metadata":{"id":"9qz4S8V2O_kc","executionInfo":{"status":"ok","timestamp":1658473931737,"user_tz":-600,"elapsed":3,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# transform the loss function to get the gradients\n","grad_fn = jax.value_and_grad(compute_loss)"],"metadata":{"id":"Br9THnrvv9VJ","executionInfo":{"status":"ok","timestamp":1658473935331,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["def _train_step(params, opt_state, input_ids, labels):\n","   # do the forward pass and get the loss and gradients\n","   loss, grads = grad_fn(params, input_ids, labels)\n","\n","   # use the gradients to update parameters\n","   updates, opt_state = opt.update(grads, opt_state)\n","   updated_params = optax.apply_updates(params, updates)\n","\n","   return updated_params, opt_state, loss\n","\n","train_step = jax.jit(_train_step)"],"metadata":{"id":"oQoBtel9wBm2","executionInfo":{"status":"ok","timestamp":1658473993923,"user_tz":-600,"elapsed":945,"user":{"displayName":"Yuji Ishikawa","userId":"17475269684774678602"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# train loop\n","for i in range(2):\n","   params, opt_state, loss = train_step(params, opt_state, train_input_ids, train_label)"],"metadata":{"id":"OmXNzwE8wR0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = TrainState.create(\n","    apply_fn=model.__call__,\n","    params=model.params,\n","    tx=opt,\n","    logits_function=eval_function,\n","    loss_function=loss_function,\n",")"],"metadata":{"id":"tfEhqkxGPYfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = model.(train_input_ids[0])\n","print(out)"],"metadata":{"id":"hjtVxyp1uCtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" optax.softmax_cross_entropy(out.logits, onehot(train_label[0], num_classes=num_labels))"],"metadata":{"id":"gK6BSXgriG6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validation"],"metadata":{"id":"CpxQhQs3RerZ"}},{"cell_type":"code","source":["test = pd.read_csv(\"./kaggle/test.csv\") "],"metadata":{"id":"8omD1G1mRhXa"},"execution_count":null,"outputs":[]}]}